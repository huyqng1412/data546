{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c9d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generate some example data\n",
    "X = 2 * np.random.rand(100, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 2).astype(int)  # Example: classify based on sum > 2\n",
    "X = np.c_[np.ones((100, 1)), X] # Add bias term (x0 = 1)\n",
    "y = (y + (np.random.rand(100) > 0.5).astype(int)) % 2 # Make it binary, but add some noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279b638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the target labels for Softmax\n",
    "num_classes = 2\n",
    "y_one_hot = np.zeros((len(y), num_classes))\n",
    "y_one_hot[np.arange(len(y)), y] = 1\n",
    "\n",
    "# Split into training and validation sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_one_hot_val = y_one_hot[:split_index], y_one_hot[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b581df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent parameters\n",
    "learning_rate = 0.1\n",
    "n_epochs = 1000\n",
    "batch_size = len(X_train) # Full batch\n",
    "m = batch_size\n",
    "best_loss = np.inf # Correctly using np.inf\n",
    "best_theta = None\n",
    "patience = 50\n",
    "patience_counter = 0\n",
    "\n",
    "# Initialize theta (weights) randomly\n",
    "theta = np.random.randn(X_train.shape[1], num_classes) # Correct initialization\n",
    "\n",
    "# Softmax function\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits)\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-10), axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "390e6464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 99!\n",
      "Training finished.\n",
      "Final weights (theta):\n",
      "[[-0.19917553 -1.04924409]\n",
      " [ 0.13365938 -0.22472968]\n",
      " [-0.71596593  0.31697102]]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Compute logits\n",
    "    logits = X_train.dot(theta)\n",
    "    # Compute predictions using softmax\n",
    "    y_pred = softmax(logits)\n",
    "    # Compute gradient\n",
    "    error = y_pred - y_train\n",
    "    gradient = (1/m) * X_train.T.dot(error)\n",
    "    # Update weights\n",
    "    theta -= learning_rate * gradient\n",
    "\n",
    "    # Early stopping check on validation set\n",
    "    val_logits = X_val.dot(theta)\n",
    "    val_y_pred = softmax(val_logits)\n",
    "    val_loss = cross_entropy_loss(y_one_hot_val, val_y_pred)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_theta = theta\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}!\")\n",
    "            break\n",
    "\n",
    "print(\"Training finished.\")\n",
    "if best_theta is not None:\n",
    "    print(\"Final weights (theta):\")\n",
    "    print(best_theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56265889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for X_new: [1], probabilities: [[0.45964962 0.54035038]]\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "X_new = np.array([[1, 1.5, 1.5]]) # Use the same bias term format\n",
    "probabilities = softmax(X_new.dot(best_theta))\n",
    "prediction = np.argmax(probabilities, axis=1)\n",
    "print(f\"Prediction for X_new: {prediction}, probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230347ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
